{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452b21f6-8f6b-4b7d-ab28-975b4a0e2742",
   "metadata": {},
   "source": [
    "## Aditi Nabar Take Home Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1083bf1-4647-4742-87ec-3a4d19c42ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "annotated_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668471de-777f-4b49-a121-334159991ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d458bd1d-b5c6-430c-9ff7-32e3dfe5d5a0",
   "metadata": {},
   "source": [
    "### Step 1: Implement a Sentence Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6a5b3a-d214-4a38-83d2-24b75d189bdd",
   "metadata": {},
   "source": [
    "#### Transformer Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0999d434-55d4-4a37-a30e-9937d5e77fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTTransformer(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(BERTTransformer, self).__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, **kwargs):\n",
    "        # model is initialized without decoder\n",
    "        # get the cls token embedding from the last hidden state, basically the sentence embeddings from encoder.\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embedding = output.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca4ff56-14c2-4eb5-94a2-9533daaf3705",
   "metadata": {},
   "source": [
    "Above we have a basic model class that loads a pre-trained model, and can generate a sentence embedding using the .forward() method. Below are some helper functions you can use to pass in a sentence and return an embedding. These helper functions can also be used with the class created below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100b9913-7eeb-43d1-aad6-b370919e06ae",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ecae721-3280-435e-b6fd-29722d43919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy dataset if you want sentences to pass\n",
    "annotated_df = pd.read_csv(\"annotated_w_quotes.csv\").rename({\"annotations\": \"ner_annotations\"}, axis=1)\n",
    "included_cols = [\"headline\", \"short_description\"]\n",
    "classification_label_col = \"category\"\n",
    "ner_label_col = \"ner_annotations\"\n",
    "\n",
    "def get_input_sentence(row, included_columns=None):\n",
    "    \"\"\"Helper to generate sentences at row level on a dataframe, or just returns a sentence\"\"\"\n",
    "    if isinstance(row, pd.Series):\n",
    "        return ' '.join([row[col] for col in included_columns])\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "def generate_sentence_embeddings(model, input_text, included_columns=None, task=None):\n",
    "    # build input sentence\n",
    "    sentence = get_input_sentence(input_text, included_columns)\n",
    "\n",
    "    # get encoded input sentence\n",
    "    encoded_input = tokenizer(sentence, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    # get bert sentence embedding with contextual understanding\n",
    "    bert_sent_embedding = model.forward(encoded_input[\"input_ids\"], attention_mask=encoded_input[\"attention_mask\"], task=task)\n",
    "    return bert_sent_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3085e46b-94c1-4827-a459-0d94a5f0e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "basicBERTmodel = BERTTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42be9e50-50bd-4fcf-8956-07f010e4965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    record = annotated_df.iloc[i]\n",
    "    embedding = generate_sentence_embeddings(basicBERTmodel, record, included_columns=included_cols)\n",
    "    print(embedding.shape)\n",
    "    # print(embedding[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5530c9aa-a8e1-46fe-99b8-d64f91772f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "tensor([ 3.2509e-02,  1.1863e-01, -7.8482e-02, -3.7044e-02, -7.4845e-02,\n",
      "        -1.2509e-01,  8.3830e-02,  4.3325e-01,  6.2973e-02, -6.9573e-02,\n",
      "         2.0910e-02,  3.4076e-02, -7.5872e-02,  1.8824e-02,  1.4187e-01,\n",
      "         8.7889e-02, -1.8660e-01,  3.9010e-01,  3.9392e-02, -2.0885e-01,\n",
      "        -2.0547e-02, -1.0078e-01, -7.3176e-02, -1.0138e-01,  1.9656e-01,\n",
      "        -1.1594e-01, -3.9264e-02, -7.7164e-02, -1.5536e-01,  2.4024e-01,\n",
      "         2.0571e-01,  2.7743e-04, -1.4508e-01,  1.3075e-01,  1.3907e-02,\n",
      "        -8.1446e-04,  1.4078e-02, -3.3752e-02,  7.1549e-02, -2.4380e-03,\n",
      "        -5.2628e-02, -7.7874e-02,  3.4912e-01, -1.1846e-01, -6.4522e-03,\n",
      "        -1.5248e-01, -1.6053e+00,  1.4721e-02, -2.4472e-01, -2.0926e-01],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# If you are passing in a text string, you don't need to pass a second argument.\n",
    "embedding = generate_sentence_embeddings(basicBERTmodel, \"INSERT SENTENCE HERE\")\n",
    "print(embedding.shape)\n",
    "print(embedding[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f09e7-193b-4c3c-9a27-388484b16bb2",
   "metadata": {},
   "source": [
    "### Step 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d9e2c1-b392-4f7d-8348-b5ff06ca3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_label_list(row):\n",
    "#     return ast.literal_eval(row[\"ner_annotations\"].replace(\"‘\", \"'\").replace(\"’\", \"'\"))\n",
    "\n",
    "num_ner_labels = 5\n",
    "num_classification_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee74f01-8d39-4f6e-8dea-1981cf66a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_category_classes, num_ner_entities):\n",
    "        super(MultiTaskBERT, self).__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.sentence_classifier_head = nn.Linear(self.bert.config.hidden_size, num_category_classes)\n",
    "        self.ner_head = nn.Linear(self.bert.config.hidden_size, num_ner_entities)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task):\n",
    "        # model is initialized without decoder\n",
    "        # get the cls token embedding from the last hidden state, basically the sentence embeddings from encoder.\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # use mean pooling of the hidden state embeddings instead of the cls token embedding given by output.pooler_output\n",
    "        if task == \"classification\":\n",
    "            pooled_sentence_embedding = self._mean_pooling(output.last_hidden_state, attention_mask)\n",
    "            task_output = self.sentence_classifier_head(pooled_sentence_embedding)\n",
    "\n",
    "        elif task == \"ner\":\n",
    "            task_output = self.ner_head(output.last_hidden_state)\n",
    "        \n",
    "        return task_output\n",
    "\n",
    "    def _mean_pooling(self, hidden_states, attention_mask):\n",
    "        # expand the mask to the dimensions of the hidden states\n",
    "        expanded_mask = attention_mask.unsqueeze(-1).expand(hidden_states.size())\n",
    "\n",
    "        # sum embeddings along the dimension of the tokens so that our output remains a vector\n",
    "        sum_embeddings = torch.sum(hidden_states * expanded_mask, dim=1)\n",
    "        sum_mask = expanded_mask.sum(dim=1)\n",
    "        return sum_embeddings / sum_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67eb4d-6283-4fdf-ab59-db85e16cdee3",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "\n",
    "On lines 6 and 7, you can see the addition of two task heads - one for the sentence classification task, and one for an NER task. These are defined in the `__init__` and then implemented in the `.forward`. \n",
    "\n",
    "A few things to note: \n",
    "- I've added linear layers as the task heads, to take the input embeddings from their higher dimensional space to the lower dimensional space of the output.\n",
    "- For the sentence embedding used by the sentence classification head, I had two options - I could use the token embedding for the CLS token which the BERT model adds to the beginning of the input sentence, which is _a_ representation of the sentence, but apparently not a good one. The other option was to perform a mean pooling on all the layers in the last hidden state, and use that as the sentence embedding. The mean pooling works by condensing the weights along the dimension of the tokens, and then applying the attention mask to enhance semantic representation. I've chosen to go the route of mean-pooling the last hidden state layers, and then use that embedding as the sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db98b561-77cf-4f4b-a852-faf6a5e37ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtlBERT = MultiTaskBERT(\n",
    "    model_name=model_name,\n",
    "    num_category_classes=num_classification_labels,\n",
    "    num_ner_entities=num_ner_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084396cc-d1f0-481d-b4c0-ff9aa8e15a7d",
   "metadata": {},
   "source": [
    "#### Generate NER predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a38ef44-a020-41cc-a613-a9cde00f60da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "tensor([[ 0.6581, -0.3127,  0.2183, -0.3534,  0.2040],\n",
      "        [ 0.0270, -0.2295,  0.1007, -0.0386,  0.2704],\n",
      "        [ 0.0393, -0.1144, -0.1133,  0.1671,  0.2099],\n",
      "        [-0.0279, -0.1757,  0.0821, -0.5505,  0.4746],\n",
      "        [-0.1393, -0.2641,  0.0574,  0.4134, -0.4826]],\n",
      "       grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# If you are passing in a text string, you don't need to pass the included_columns argument.\n",
    "embedding = generate_sentence_embeddings(mtlBERT, \"INSERT SENTENCE HERE\", task=\"ner\")\n",
    "print(embedding.shape)\n",
    "print(embedding[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94643319-e30a-4de6-8440-8d2c80367b3a",
   "metadata": {},
   "source": [
    "#### Generate sentence classification predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6a4d32b-e9db-4303-85bb-bf4bfb6e269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([-0.0495, -0.0392,  0.1757, -0.1240,  0.0898,  0.0321, -0.2603, -0.0134,\n",
      "         0.1902, -0.2885], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# If you are passing in a text string, you don't need to pass the included_columns argument.\n",
    "embedding = generate_sentence_embeddings(mtlBERT, \"INSERT SENTENCE HERE\", task=\"classification\")\n",
    "print(embedding.shape)\n",
    "print(embedding[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b96c9-194c-43a9-baf6-350efad7d704",
   "metadata": {},
   "source": [
    "### Step 3: Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47523e-8466-46b2-9324-dbf616bcde5d",
   "metadata": {},
   "source": [
    "#### 1. How would you decide which portions of the network to train and which to keep frozen? \n",
    "- a. When would it make sense to freeze the transformer backbone and only train the task specific layers? \n",
    "- b. When would it make sense to freeze one head while training the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4e965-479e-451f-ba11-629e95022955",
   "metadata": {},
   "source": [
    "At a high level, a multi-task learning pipeline is comprised of a shared backbone with various task-specific heads that allow the model to learn features specific to each task. The general case of a multi-task learning (MTL) pipeline entails running a training loop through the backbone and each task head to get weight updates across the board. There may be times when it is appropriate to deviate from this general flow and freeze parts of the network while training others, for example when there is imbalanced amounts of data across tasks, or when the various heads have reached different levels of performance. I'll get into more specifics on how to decide a path forward below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df0e31-854f-4842-a218-f8af68c294aa",
   "metadata": {},
   "source": [
    "a. I think there are two cases in which it would make sense to freeze the transformer backbone and only train the task-specific layers of a MTL pipeline: <br>1) When there is a limited amount of high quality, annotated data, freezing the backbone and training only the task-specific layers could help mitigate over-fitting by reducing the number of parameters being trained (ie only training the small amount of features in the task head rather than the millions of parameters in the transformer).  <br>2) When you don't want to interfere with the learned language understanding of the transformer backbone because it could be useful to the specialized task being learned. To explain this conversely, the more complex your task, the more layers of the pre-trained model you will want to unfreeze to provide additional capacity for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e2cae-bc05-4dad-a3a3-e23b34cc0a27",
   "metadata": {},
   "source": [
    "b. I think this question is in some ways an extension of the previous. It would make sense to freeze some heads while training others when you have limited data available for certain tasks and don't actually want to conduct any training, but still have the transformer provide the prior learning while using the frozen task head for inference, and still want the other task to benefit from a multi-task setup.  Additionally, it could be that you've achieved some acceptable level of performance on a task head but want to continue training another task head. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb74cc1-bcb3-4850-a057-0ef61a204141",
   "metadata": {},
   "source": [
    "#### 2. Discuss how you would decide when to implement a multi-task model like the one in this assignment and when it would make more sense to use two completely separate models for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a921e1a-21f5-4c6e-92ce-22440562c53a",
   "metadata": {},
   "source": [
    "I would use a multi-task model when I want to train multiple tasks that are somewhat related because they can benefit from shared feature representation. Since they will share a transformer backbone, the selection of backbone should be relevant to the various tasks I want the model to learn. Conversely, the various tasks should be appropriate given the selection of the transformer backbone. \n",
    "\n",
    "1. Task type<br>\n",
    "It would make sense to combine tasks that operate and learn at the same level of granularity into one model, because the model will be learning a representation specific to that kind of task. For example, tasks that operate at the token level (ie various document NER tasks) would be good candidates for a shared MTL model. Tasks that are at the document level (ex. document classification, document pair similarity tasks, etc) would be good candidates for another shared MTL model. \n",
    "\n",
    "2. Data availability<br>\n",
    "If the size of datasets for a set of similar tasks is quite imbalanced, and if you do want to train a model for each task as opposed to freezing a head as discussed earlier, it may make more sense to train models separately for those tasks so as to not have the model overfit the smaller-dataset task while it trains the other.\n",
    "\n",
    "3. Differing modalities<br>\n",
    "The model architecture of a pre-trained transformer backbone needed for one modality is going to be different from that needed for another modality, thus tasks of differing modalities should be trained as separate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7a569-6085-43b4-9990-ba287bb7ada6",
   "metadata": {},
   "source": [
    "#### 3. When training the multi-task model, assume that Task A has abundant data, while Task B has limited data. Explain how you would handle this imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce6c5a-9587-42c9-b3be-4f145c6e6cd5",
   "metadata": {},
   "source": [
    "As discussed above, when there is a data imbalance as described in this question, I would propose two options: 1) freeze the head of Task B, and train Task A. Evaluate model performance of each head. If Task B has the desired performance off the bat using the pre-trained weights and untrained task head for inference, you could keep them as an MTL model and continue training Task A.  2) If the Task B head is not performant, then it may make sense to split the tasks into separate models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flair_env_39",
   "language": "python",
   "name": "flair_env_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
